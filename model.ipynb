{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 directories and 0 images in 'C:\\Users\\crack\\Downloads\\wheat_disease\\data\\train'.\n",
      "There are 0 directories and 902 images in 'C:\\Users\\crack\\Downloads\\wheat_disease\\data\\train\\Brown_rust'.\n",
      "There are 0 directories and 1116 images in 'C:\\Users\\crack\\Downloads\\wheat_disease\\data\\train\\Healthy'.\n",
      "There are 0 directories and 924 images in 'C:\\Users\\crack\\Downloads\\wheat_disease\\data\\train\\Yellow_rust'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "# Walk through  directory and list number of files\n",
    "for dirpath, dirnames, filenames in os.walk(\"C:\\\\Users\\\\crack\\\\Downloads\\\\wheat_disease\\\\data\\\\train\"):\n",
    "  print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from PIL import Image\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "from keras.layers import Conv2D, Flatten, Dense, MaxPooling2D, Dropout\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.applications import EfficientNetB3\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Brown_rust', 'Healthy', 'Yellow_rust']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir='C:\\\\Users\\\\crack\\\\Downloads\\\\wheat_disease\\\\data\\\\train'\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = ['Brown_rust', 'Healthy', 'Yellow_rust']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2355 images belonging to 3 classes.\n",
      "Found 587 images belonging to 3 classes.\n",
      "Found 368 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Directories for dataset\n",
    "TRAIN_DIR = 'C:\\\\Users\\\\crack\\\\Downloads\\\\wheat_disease\\\\data\\\\train'\n",
    "VAL_DIR = 'C:\\\\Users\\\\crack\\\\Downloads\\\\wheat_disease\\\\data\\\\val'\n",
    "\n",
    "# Image size and batch size\n",
    "IMAGE_SIZE = [224, 224]\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Define class labels\n",
    "class_labels = ['Healthy', 'Brown_rust', 'Yellow_rust']\n",
    "\n",
    "# Data augmentation and preprocessing for train and validation split\n",
    "datagen_train = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2  # 80% training, 20% validation\n",
    ")\n",
    "\n",
    "datagen_val_test = ImageDataGenerator(rescale=1./255, validation_split=0.5)  # 50% validation, 50% test\n",
    "\n",
    "# Creating train, validation, and test generators\n",
    "train_generator = datagen_train.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "val_generator = datagen_train.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "test_generator = datagen_val_test.flow_from_directory(\n",
    "    VAL_DIR,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False,\n",
    "    seed=42,\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Load EfficientNetB0 without the top layer\n",
    "base_model = EfficientNetB3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "\n",
    "# Unfreeze last few layers of the base model for fine-tuning\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-20]:  # Freeze only the first few layers\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom classification layers\n",
    "x = GlobalAveragePooling2D()(base_model.output)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.4)(x)  # Adding dropout to prevent overfitting\n",
    "output = Dense(len(class_labels), activation='softmax')(x)  # 5 output classes\n",
    "\n",
    "# Create model\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=AdamW(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(train_generator.classes),\n",
    "    y=train_generator.classes\n",
    ")\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# Add EarlyStopping to avoid overfitting\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\wheat_disease\\.venv\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 3s/step - accuracy: 0.5531 - loss: 0.9398 - val_accuracy: 0.3799 - val_loss: 1.0496 - learning_rate: 1.0000e-04\n",
      "Epoch 2/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 2s/step - accuracy: 0.6195 - loss: 0.8366 - val_accuracy: 0.5537 - val_loss: 0.9948 - learning_rate: 1.0000e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 3s/step - accuracy: 0.6057 - loss: 0.8458 - val_accuracy: 0.6167 - val_loss: 0.8928 - learning_rate: 1.0000e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m282s\u001b[0m 4s/step - accuracy: 0.6458 - loss: 0.7918 - val_accuracy: 0.6269 - val_loss: 0.8420 - learning_rate: 1.0000e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m283s\u001b[0m 4s/step - accuracy: 0.6536 - loss: 0.7794 - val_accuracy: 0.6831 - val_loss: 0.7503 - learning_rate: 1.0000e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m298s\u001b[0m 4s/step - accuracy: 0.6650 - loss: 0.7632 - val_accuracy: 0.4889 - val_loss: 0.8508 - learning_rate: 1.0000e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 4s/step - accuracy: 0.6600 - loss: 0.7521 - val_accuracy: 0.6917 - val_loss: 0.7545 - learning_rate: 1.0000e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 3s/step - accuracy: 0.6549 - loss: 0.7582 - val_accuracy: 0.6252 - val_loss: 0.7752 - learning_rate: 1.0000e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 3s/step - accuracy: 0.6967 - loss: 0.7083 - val_accuracy: 0.6865 - val_loss: 0.6862 - learning_rate: 2.0000e-05\n",
      "Epoch 10/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 3s/step - accuracy: 0.6882 - loss: 0.7054 - val_accuracy: 0.7530 - val_loss: 0.6247 - learning_rate: 2.0000e-05\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=EPOCHS,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[early_stopping,lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - accuracy: 0.7038 - loss: 0.6493\n",
      "Test Accuracy: 0.7147\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_acc = model.evaluate(test_generator, verbose=1)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# 13. Save the model\n",
    "model.save(\"wheat_disease_fixed.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
